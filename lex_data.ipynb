{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some tests for lexer\n",
    "\n",
    "test_query_1 = \"SELECT * /* QUERY_GROUP_ID:main_dashboard_top_query */ from table where value = 'Palo Alto';\"\n",
    "\n",
    "test_query_2 = \" SELECT  * /* QUERY_GROUP_ID:main_dashboard_top_query */  from table where    value = 'Palo Alto' and value != '\\\"KEK\\\"' ; /* LOL */\"\n",
    "\n",
    "\n",
    "test_tokens_file = \"lexer_tokens.txt\"\n",
    "test_tokens_types_file = \"lexer_tokens_types.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_tokens_file, \"w+\") as _, open(test_tokens_types_file, \"w+\") as _:\n",
    "    pass\n",
    "\n",
    "command = ['./lexer', test_tokens_types_file, test_tokens_file]\n",
    "input_line = test_query_1.strip()\n",
    "process = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "process.stdin.write(input_line.encode())\n",
    "process.stdin.close()\n",
    "process.wait()\n",
    "\n",
    "# I used char(31), because we need to separate tokens and string literals can be tricky to separate, they need to go as one\n",
    "tokens = []\n",
    "tokens_types = []\n",
    "with open(test_tokens_file, \"rb\") as f:\n",
    "    tokens = list(map(lambda x: x.decode(\"utf-8\"), f.readline().split(b\"\\x1f\")))\n",
    "\n",
    "with open(test_tokens_types_file, \"rb\") as f:\n",
    "    tokens_types = list(map(lambda x: x.decode(\"utf-8\"), f.readline().split(b\"\\x1f\")))\n",
    "\n",
    "tokens_expected = [\n",
    "    \"SELECT\", \"*\", \"from\", \"table\", \"where\", \"value\", \"=\", \"'Palo Alto'\", \";\", \"\\n\"\n",
    "]\n",
    "\n",
    "\n",
    "assert len(tokens) == len(tokens_types) == len(tokens_expected)\n",
    "\n",
    "assert tokens == tokens_expected, \"Tokens do not match the expected byte string.\"\n",
    "\n",
    "tokens_types_expected = [\n",
    "     \"BareWord\", \"Asterisk\", \"BareWord\", \"BareWord\", \"BareWord\", \"BareWord\", \"Equals\", \"StringLiteral\", \"Semicolon\", \"\\n\"\n",
    "]\n",
    "\n",
    "assert len(tokens) == len(tokens_types) == len(tokens_expected) == len(tokens_types_expected)\n",
    "\n",
    "assert tokens_types == tokens_types_expected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_tokens_file, \"w+\") as _, open(test_tokens_types_file, \"w+\") as _:\n",
    "    pass\n",
    "\n",
    "command = ['./lexer', test_tokens_types_file, test_tokens_file]\n",
    "input_line = test_query_2.strip()\n",
    "process = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "process.stdin.write(input_line.encode())\n",
    "process.stdin.close()\n",
    "process.wait()\n",
    "\n",
    "# I used char(31), because we need to separate tokens and string literals can be tricky to separate, they need to go as one\n",
    "tokens = []\n",
    "tokens_types = []\n",
    "with open(test_tokens_file, \"rb\") as f:\n",
    "    tokens = list(map(lambda x: x.decode(\"utf-8\"), f.readline().split(b\"\\x1f\")))\n",
    "\n",
    "with open(test_tokens_types_file, \"rb\") as f:\n",
    "    tokens_types = list(map(lambda x: x.decode(\"utf-8\"), f.readline().split(b\"\\x1f\")))\n",
    "\n",
    "tokens_expected = [\n",
    "    \"SELECT\", \"*\", \"from\", \"table\", \"where\", \"value\", \"=\", \"'Palo Alto'\", \"and\", \"value\", \"!=\", \"'\\\"KEK\\\"'\", \";\", \"\\n\"\n",
    "]\n",
    "\n",
    "\n",
    "assert tokens == tokens_expected, \"Tokens do not match the expected byte string.\"\n",
    "\n",
    "tokens_types_expected = ['BareWord', 'Asterisk', 'BareWord', 'BareWord', 'BareWord', 'BareWord', 'Equals', 'StringLiteral', 'BareWord', 'BareWord', 'NotEquals', 'StringLiteral', 'Semicolon', '\\n']\n",
    "\n",
    "assert len(tokens) == len(tokens_types) == len(tokens_expected) == len(tokens_types_expected)\n",
    "\n",
    "assert tokens_types == tokens_types_expected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_lexer/validation/tokens/synth_validation.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516142de82e74d4c9cd12f4d794e3ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cleaned_data/validation/synth_validation.txt: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_lexer/validation/tokens/clickhouse_validation_pr_60095.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32022ff5913406e8a13a507bf6ef91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cleaned_data/validation/clickhouse_validation_pr_60095.txt: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_lexer/validation/tokens/WorldSoccerDataBase.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0adcb51bb76449c596b7b005d0199589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cleaned_data/validation/WorldSoccerDataBase.txt: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_lexer/validation/tokens/USWildFires.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ea544a64524d378d12d2051eb774d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cleaned_data/validation/USWildFires.txt: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_lexer/validation/tokens/wikisql_validation.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d223032037b2489c805d014bb3f4729b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cleaned_data/validation/wikisql_validation.txt: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_lexer/validation/tokens/spider_test.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a960d6875d4c169586888bfd431f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cleaned_data/validation/spider_test.txt: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_lexer/test/tokens/synth_test.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456466a3cf7b4c5c9ac9ba418fd183be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cleaned_data/test/synth_test.txt: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_lexer/test/tokens/clickhouse_test_pr_60095.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2f3d8818e44ba7bb12222eff49f666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cleaned_data/test/clickhouse_test_pr_60095.txt: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_lexer/test/tokens/WhatCDHipHop.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29899ad63d854ebf8d7cd15c9f7b97e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cleaned_data/test/WhatCDHipHop.txt: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_lexer/test/tokens/wikisql_test.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f5b3aa399042789b8d9491bd93f4e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cleaned_data/test/wikisql_test.txt: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_lexer/train/tokens/GreaterManchesterCrime.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db19d199faae48b9a766c6c8a309db96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cleaned_data/train/GreaterManchesterCrime.txt: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_lexer/train/tokens/synth_train.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4eb0cbad5fb4ae79748bc107d4df3a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cleaned_data/train/synth_train.txt: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_lexer/train/tokens/KaggleDBQA_combined.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69244bd59fb9490c9277ca53727d4ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cleaned_data/train/KaggleDBQA_combined.txt: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_lexer/train/tokens/TheHistoryofBaseball.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b9df588f2846e38dd83c69eb5940fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cleaned_data/train/TheHistoryofBaseball.txt: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_lexer/train/tokens/clickhouse_train_pr_60095.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38b93523fcc4e3b9f582de34b71d254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cleaned_data/train/clickhouse_train_pr_60095.txt: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_lexer/train/tokens/GeoNuclearData.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e593beecefb4fc5ad29722286e3e252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cleaned_data/train/GeoNuclearData.txt: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_lexer/train/tokens/Pesticide.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca95dee403541238aab5142e6566681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cleaned_data/train/Pesticide.txt: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_lexer/train/tokens/spider_train.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a1d07254994684969034e8ed9502dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cleaned_data/train/spider_train.txt: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_lexer/train/tokens/StudentMathScore.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d54e4de76845a786046344c5ecc771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cleaned_data/train/StudentMathScore.txt: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_lexer/train/tokens/wikisql_train.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3c7b5869a3493083545a939fa5751b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cleaned_data/train/wikisql_train.txt: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lex all queries and get token types (see after_lexer folder)\n",
    "for walk_obj in os.walk(\"cleaned_data\"):\n",
    "    if len(walk_obj[1]) != 0:\n",
    "        continue\n",
    "    folder_name = walk_obj[0]\n",
    "    for file_name in walk_obj[2]:\n",
    "        path_cleaned = os.path.join(folder_name, file_name)\n",
    "        tokens_file = os.path.join(\"after_lexer/\", folder_name.split(\"cleaned_data/\")[1], \"tokens/\", file_name)\n",
    "        print(tokens_file)\n",
    "        tokens_type_file = os.path.join(\"after_lexer/\", folder_name.split(\"cleaned_data/\")[1], \"tokens_types/\", file_name)\n",
    "        with open(path_cleaned, 'r') as cleaned_file:\n",
    "            i = 1\n",
    "            for line in tqdm(cleaned_file, desc=path_cleaned):\n",
    "                command = ['./lexer', tokens_type_file, tokens_file]\n",
    "                input_line = line.strip()\n",
    "                process = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "                process.stdin.write(input_line.encode())\n",
    "                process.stdin.close()\n",
    "                process.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = set([\n",
    "    \"ACCESS\",\n",
    "    \"ACTION\",\n",
    "    \"ADD\",\n",
    "    \"ADMIN\",\n",
    "    \"AFTER\",\n",
    "    \"ALGORITHM\",\n",
    "    \"ALIAS\",\n",
    "    \"ALL\",\n",
    "    \"ALLOWED_LATENESS\",\n",
    "    \"ALTER\",\n",
    "    \"AND\",\n",
    "    \"ANTI\",\n",
    "    \"ANY\",\n",
    "    \"APPLY\",\n",
    "    \"ARRAY\",\n",
    "    \"AS\",\n",
    "    \"ASC\",\n",
    "    \"ASCENDING\",\n",
    "    \"ASOF\",\n",
    "    \"ASSUME\",\n",
    "    \"AST\",\n",
    "    \"ASYNC\",\n",
    "    \"ATTACH\",\n",
    "    \"AUTO_INCREMENT\",\n",
    "    \"BACKUP\",\n",
    "    \"BASE_BACKUP\",\n",
    "    \"BEGIN\",\n",
    "    \"BETWEEN\",\n",
    "    \"BIDIRECTIONAL\",\n",
    "    \"BOTH\",\n",
    "    \"BY\",\n",
    "    \"CACHE\",\n",
    "    \"CACHES\",\n",
    "    \"CASCADE\",\n",
    "    \"CASE\",\n",
    "    \"CASEWITHEXPRESSION\",\n",
    "    \"CAST\",\n",
    "    \"CHANGE\",\n",
    "    \"CHANGEABLE_IN_READONLY\",\n",
    "    \"CHANGED\",\n",
    "    \"CHAR\",\n",
    "    \"CHARACTER\",\n",
    "    \"CHECK\",\n",
    "    \"CLEANUP\",\n",
    "    \"CLEAR\",\n",
    "    \"CLUSTER\",\n",
    "    \"CLUSTER_HOST_IDS\",\n",
    "    \"CLUSTERS\",\n",
    "    \"CN\",\n",
    "    \"CODEC\",\n",
    "    \"COLLATE\",\n",
    "    \"COLLECTION\",\n",
    "    \"COLUMN\",\n",
    "    \"COLUMNS\",\n",
    "    \"COMMENT\",\n",
    "    \"COMMIT\",\n",
    "    \"COMPRESSION\",\n",
    "    \"CONCAT\",\n",
    "    \"CONSTRAINT\",\n",
    "    \"CREATE\",\n",
    "    \"CROSS\",\n",
    "    \"CUBE\",\n",
    "    \"CURRENT\",\n",
    "    \"CURRENT_USER\",\n",
    "    \"DATABASE\",\n",
    "    \"DATABASES\",\n",
    "    \"DATE\",\n",
    "    \"DATE_ADD\",\n",
    "    \"DATEADD\",\n",
    "    \"DATE_DIFF\",\n",
    "    \"DATEDIFF\",\n",
    "    \"DATE_SUB\",\n",
    "    \"DATESUB\",\n",
    "    \"DAY\",\n",
    "    \"DD\",\n",
    "    \"DDL\",\n",
    "    \"DEDUPLICATE\",\n",
    "    \"DEFAULT\",\n",
    "    \"DELAY\",\n",
    "    \"DELETE\",\n",
    "    \"DESC\",\n",
    "    \"DESCENDING\",\n",
    "    \"DESCRIBE\",\n",
    "    \"DETACH\",\n",
    "    \"DETACHED\",\n",
    "    \"DICTIONARIES\",\n",
    "    \"DICTIONARY\",\n",
    "    \"DISK\",\n",
    "    \"DISTINCT\",\n",
    "    \"DIV\",\n",
    "    \"DOUBLE_SHA1_HASH\",\n",
    "    \"DROP\",\n",
    "    \"ELSE\",\n",
    "    \"EMPTY\",\n",
    "    \"ENABLED\",\n",
    "    \"END\",\n",
    "    \"ENFORCED\",\n",
    "    \"ENGINE\",\n",
    "    \"EPHEMERAL\",\n",
    "    \"EQUALS\",\n",
    "    \"ESTIMATE\",\n",
    "    \"EVENT\",\n",
    "    \"EVENTS\",\n",
    "    \"EXCEPT\",\n",
    "    \"EXCHANGE\",\n",
    "    \"EXISTS\",\n",
    "    \"EXPLAIN\",\n",
    "    \"EXPRESSION\",\n",
    "    \"EXTERNAL\",\n",
    "    \"EXTRACT\",\n",
    "    \"FALSE\",\n",
    "    \"FETCH\",\n",
    "    \"FILE\",\n",
    "    \"FILESYSTEM\",\n",
    "    \"FILL\",\n",
    "    \"FILTER\",\n",
    "    \"FINAL\",\n",
    "    \"FIRST\",\n",
    "    \"FOLLOWING\",\n",
    "    \"FOR\",\n",
    "    \"FOREIGN\",\n",
    "    \"FORMAT\",\n",
    "    \"FREEZE\",\n",
    "    \"FROM\",\n",
    "    \"FULL\",\n",
    "    \"FULLTEXT\",\n",
    "    \"FUNCTION\",\n",
    "    \"GLOBAL\",\n",
    "    \"GRANT\",\n",
    "    \"GRANTEES\",\n",
    "    \"GRANTS\",\n",
    "    \"GRANULARITY\",\n",
    "    \"GREATER\",\n",
    "    \"GREATEROREQUALS\",\n",
    "    \"GROUP\",\n",
    "    \"GROUPING\",\n",
    "    \"GROUPS\",\n",
    "    \"HASH\",\n",
    "    \"HAVING\",\n",
    "    \"HDFS\",\n",
    "    \"HH\",\n",
    "    \"HIERARCHICAL\",\n",
    "    \"HOST\",\n",
    "    \"HOUR\",\n",
    "    \"ID\",\n",
    "    \"IDENTIFIED\",\n",
    "    \"IF\",\n",
    "    \"ILIKE\",\n",
    "    \"IN\",\n",
    "    \"INDEX\",\n",
    "    \"INFILE\",\n",
    "    \"INHERIT\",\n",
    "    \"INJECTIVE\",\n",
    "    \"INNER\",\n",
    "    \"INSERT\",\n",
    "    \"INTERPOLATE\",\n",
    "    \"INTERSECT\",\n",
    "    \"INTERVAL\",\n",
    "    \"INTO\",\n",
    "    \"INVISIBLE\",\n",
    "    \"IP\",\n",
    "    \"IS\",\n",
    "    \"IS_OBJECT_ID\",\n",
    "    \"JOIN\",\n",
    "    \"KEY\",\n",
    "    \"KEYED\",\n",
    "    \"KILL\",\n",
    "    \"LAMBDA\",\n",
    "    \"LARGE\",\n",
    "    \"LAST\",\n",
    "    \"LAYOUT\",\n",
    "    \"LEADING\",\n",
    "    \"LEFT\",\n",
    "    \"LESS\",\n",
    "    \"LESSOREQUALS\",\n",
    "    \"LEVEL\",\n",
    "    \"LIFETIME\",\n",
    "    \"LIKE\",\n",
    "    \"LIMIT\",\n",
    "    \"LIMITS\",\n",
    "    \"LINEAR\",\n",
    "    \"LIST\",\n",
    "    \"LITERAL\",\n",
    "    \"LIVE\",\n",
    "    \"LOCAL\",\n",
    "    \"LTRIM\",\n",
    "    \"MATCH\",\n",
    "    \"MATERIALIZE\",\n",
    "    \"MATERIALIZED\",\n",
    "    \"MAX\",\n",
    "    \"MCS\",\n",
    "    \"MEMORY\",\n",
    "    \"MI\",\n",
    "    \"MICROSECOND\",\n",
    "    \"MILLISECOND\",\n",
    "    \"MIN\",\n",
    "    \"MINUS\",\n",
    "    \"MINUTE\",\n",
    "    \"MM\",\n",
    "    \"MOD\",\n",
    "    \"MODIFY\",\n",
    "    \"MONTH\",\n",
    "    \"MOVE\",\n",
    "    \"MS\",\n",
    "    \"MULTIIF\",\n",
    "    \"MUTATION\",\n",
    "    \"NAME\",\n",
    "    \"NAMED\",\n",
    "    \"NANOSECOND\",\n",
    "    \"NEXT\",\n",
    "    \"NO\",\n",
    "    \"NONE\",\n",
    "    \"NOT\",\n",
    "    \"NOTEQUALS\",\n",
    "    \"NOTIN\",\n",
    "    \"NS\",\n",
    "    \"NULL\",\n",
    "    \"NULLS\",\n",
    "    \"OBJECT\",\n",
    "    \"OFFSET\",\n",
    "    \"ON\",\n",
    "    \"ONLY\",\n",
    "    \"OPTIMIZE\",\n",
    "    \"OPTION\",\n",
    "    \"OR\",\n",
    "    \"ORDER\",\n",
    "    \"OUTER\",\n",
    "    \"OUTFILE\",\n",
    "    \"OVER\",\n",
    "    \"OVERRIDE\",\n",
    "    \"PART\",\n",
    "    \"PARTIAL\",\n",
    "    \"PARTITION\",\n",
    "    \"PARTITIONS\",\n",
    "    \"PART_MOVE_TO_SHARD\",\n",
    "    \"PERMANENTLY\",\n",
    "    \"PERMISSIVE\",\n",
    "    \"PIPELINE\",\n",
    "    \"PLAN\",\n",
    "    \"PLUS\",\n",
    "    \"POLICY\",\n",
    "    \"POPULATE\",\n",
    "    \"POSITION\",\n",
    "    \"PRECEDING\",\n",
    "    \"PRECISION\",\n",
    "    \"PREWHERE\",\n",
    "    \"PRIMARY\",\n",
    "    \"PRIVILEGES\",\n",
    "    \"PROCESSLIST\",\n",
    "    \"PROFILE\",\n",
    "    \"PROJECTION\",\n",
    "    \"QQ\",\n",
    "    \"QUARTER\",\n",
    "    \"QUERY\",\n",
    "    \"QUOTA\",\n",
    "    \"RANDOMIZED\",\n",
    "    \"RANGE\",\n",
    "    \"READONLY\",\n",
    "    \"REALM\",\n",
    "    \"RECOMPRESS\",\n",
    "    \"REFERENCES\",\n",
    "    \"REFRESH\",\n",
    "    \"REGEXP\",\n",
    "    \"REGEXPQUOTEMETA\",\n",
    "    \"REMOVE\",\n",
    "    \"RENAME\",\n",
    "    \"REPLACE\",\n",
    "    \"REPLACEREGEXPALL\",\n",
    "    \"REPLACEREGEXPONE\",\n",
    "    \"RESET\",\n",
    "    \"RESTORE\",\n",
    "    \"RESTRICT\",\n",
    "    \"RESTRICTIVE\",\n",
    "    \"RESUME\",\n",
    "    \"REVOKE\",\n",
    "    \"RIGHT\",\n",
    "    \"ROLE\",\n",
    "    \"ROLES\",\n",
    "    \"ROLLBACK\",\n",
    "    \"ROLLUP\",\n",
    "    \"ROW\",\n",
    "    \"ROWS\",\n",
    "    \"RTRIM\",\n",
    "    \"S3\",\n",
    "    \"SALT\",\n",
    "    \"SAMPLE\",\n",
    "    \"SECOND\",\n",
    "    \"SELECT\",\n",
    "    \"SEMI\",\n",
    "    \"SERVER\",\n",
    "    \"SET\",\n",
    "    \"SETS\",\n",
    "    \"SETTING\",\n",
    "    \"SETTINGS\",\n",
    "    \"SHA256_HASH\",\n",
    "    \"SHARD\",\n",
    "    \"SHOW\",\n",
    "    \"SIGNED\",\n",
    "    \"SIMPLE\",\n",
    "    \"SINGLEVALUEORNULL\",\n",
    "    \"SNAPSHOT\",\n",
    "    \"SOURCE\",\n",
    "    \"SPATIAL\",\n",
    "    \"SS\",\n",
    "    \"STDOUT\",\n",
    "    \"STEP\",\n",
    "    \"STORAGE\",\n",
    "    \"STRICT\",\n",
    "    \"STRICTLY_ASCENDING\",\n",
    "    \"SUBPARTITION\",\n",
    "    \"SUBPARTITIONS\",\n",
    "    \"SUBSTRING\",\n",
    "    \"SUSPEND\",\n",
    "    \"SYNC\",\n",
    "    \"SYNTAX\",\n",
    "    \"SYSTEM\",\n",
    "    \"TABLE\",\n",
    "    \"TABLES\",\n",
    "    \"TEMPORARY\",\n",
    "    \"TEST\",\n",
    "    \"THAN\",\n",
    "    \"THEN\",\n",
    "    \"TIES\",\n",
    "    \"TIMESTAMP\",\n",
    "    \"TIMESTAMP_ADD\",\n",
    "    \"TIMESTAMPADD\",\n",
    "    \"TIMESTAMP_DIFF\",\n",
    "    \"TIMESTAMPDIFF\",\n",
    "    \"TIMESTAMP_SUB\",\n",
    "    \"TIMESTAMPSUB\",\n",
    "    \"TO\",\n",
    "    \"TODATE\",\n",
    "    \"TODATETIME\",\n",
    "    \"TOP\",\n",
    "    \"TOTALS\",\n",
    "    \"TRACKING\",\n",
    "    \"TRAILING\",\n",
    "    \"TRANSACTION\",\n",
    "    \"TREE\",\n",
    "    \"TRIGGER\",\n",
    "    \"TRIM\",\n",
    "    \"TRIMBOTH\",\n",
    "    \"TRIMLEFT\",\n",
    "    \"TRIMRIGHT\",\n",
    "    \"TRUE\",\n",
    "    \"TRUNCATE\",\n",
    "    \"TTL\",\n",
    "    \"TUPLE\",\n",
    "    \"TYPE\",\n",
    "    \"UNBOUNDED\",\n",
    "    \"UNFREEZE\",\n",
    "    \"UNION\",\n",
    "    \"UNIQUE\",\n",
    "    \"UNSIGNED\",\n",
    "    \"UNTUPLE\",\n",
    "    \"UPDATE\",\n",
    "    \"URL\",\n",
    "    \"USE\",\n",
    "    \"USER\",\n",
    "    \"USING\",\n",
    "    \"UUID\",\n",
    "    \"VALUES\",\n",
    "    \"VARYING\",\n",
    "    \"VIEW\",\n",
    "    \"VIEWIFPERMITTED\",\n",
    "    \"VISIBLE\",\n",
    "    \"VOLUME\",\n",
    "    \"WATCH\",\n",
    "    \"WATERMARK\",\n",
    "    \"WEEK\",\n",
    "    \"WHEN\",\n",
    "    \"WHERE\",\n",
    "    \"WINDOW\",\n",
    "    \"WITH\",\n",
    "    \"WK\",\n",
    "    \"WRITABLE\",\n",
    "    \"YEAR\",\n",
    "    \"YYYY\",\n",
    "    \"ZKPATH\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "GreaterManchesterCrime.txt\n",
      "synth_train.txt\n",
      "KaggleDBQA_combined.txt\n",
      "TheHistoryofBaseball.txt\n",
      "clickhouse_train_pr_60095.txt\n",
      "GeoNuclearData.txt\n",
      "Pesticide.txt\n",
      "spider_train.txt\n",
      "StudentMathScore.txt\n",
      "wikisql_train.txt\n",
      "test\n",
      "synth_test.txt\n",
      "clickhouse_test_pr_60095.txt\n",
      "WhatCDHipHop.txt\n",
      "wikisql_test.txt\n",
      "validation\n",
      "synth_validation.txt\n",
      "clickhouse_validation_pr_60095.txt\n",
      "WorldSoccerDataBase.txt\n",
      "USWildFires.txt\n",
      "wikisql_validation.txt\n",
      "spider_test.txt\n"
     ]
    }
   ],
   "source": [
    "# clean from errors and replace BareWord with KeyWord\n",
    "\n",
    "\n",
    "for folder in [\"train\", \"test\", \"validation\"]:\n",
    "    print(folder)\n",
    "    tokens_folder = os.path.join(\"after_lexer\", folder, \"tokens\")\n",
    "    tokens_types_folder = os.path.join(\"after_lexer\", folder, \"tokens_types\")\n",
    "    result_folder = os.path.join(\"final_dataset\", folder)\n",
    "    for file in os.listdir(tokens_folder):\n",
    "        print(file)\n",
    "        with open(os.path.join(tokens_folder, file), \"rb\") as tokens_f,\\\n",
    "             open(os.path.join(tokens_types_folder, file), \"rb\") as tokens_types_f,\\\n",
    "             open(os.path.join(result_folder, file), \"w\") as result_f:\n",
    "            \n",
    "            for tokens in tokens_f:\n",
    "                try:\n",
    "                    tokens = list(map(lambda x: x.decode(\"utf-8\"), tokens.split(b\"\\x1f\")))\n",
    "                except:\n",
    "                    next(tokens_types_f)\n",
    "                    continue\n",
    "                tokens_types = list(map(lambda x: x.decode(\"utf-8\"), tokens_types_f.readline().split(b\"\\x1f\")))\n",
    "\n",
    "                if any('error' in token_type.lower() for token_type in tokens_types):\n",
    "                    continue\n",
    "\n",
    "                if tokens[-1] == \"\\n\":\n",
    "                    tokens = tokens[:-1]\n",
    "                \n",
    "                if tokens_types[-1] == \"\\n\":\n",
    "                    tokens_types = tokens_types[:-1]\n",
    "\n",
    "                result_data = []\n",
    "\n",
    "                assert len(tokens) == len(tokens_types)\n",
    "\n",
    "                for token, token_type in zip(tokens, tokens_types):\n",
    "                    if token_type == \"BareWord\" and token.upper() in keywords:\n",
    "                        if (\"wikisql\" in file) and token.upper() == \"TABLE\":\n",
    "                            result_data.append(\"Identifier\")\n",
    "                            continue\n",
    "                        result_data.append(token.upper())\n",
    "                    elif token_type == \"BareWord\" and token.upper() not in keywords:\n",
    "                        result_data.append(\"Identifier\")\n",
    "                    else:\n",
    "                        result_data.append(token_type)\n",
    "                \n",
    "                assert len(result_data) == len(tokens) == len(tokens_types)\n",
    "                \n",
    "                result_f.write(\" \".join(result_data) + \"\\n\")\n",
    "                \n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
