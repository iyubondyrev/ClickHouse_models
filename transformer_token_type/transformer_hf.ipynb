{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from numpy import sqrt, log\n",
    "from dataset import TokenTypesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TokenTypesDataset(folder=\"../tokentype_keywordasitis_data/train\")\n",
    "val_dataset = TokenTypesDataset(folder=\"../tokentype_keywordasitis_data/validation\", train=False, vocabs=(train_dataset.token2idx, train_dataset.idx2token), max_length=train_dataset.max_length)\n",
    "test_dataset = TokenTypesDataset(folder=\"../tokentype_keywordasitis_data/test\", train=False, vocabs=(train_dataset.token2idx, train_dataset.idx2token), max_length=train_dataset.max_length)\n",
    "assert val_dataset.vocab_size == train_dataset.vocab_size == test_dataset.vocab_size\n",
    "assert val_dataset.max_length == train_dataset.max_length == test_dataset.max_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz, device):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_masks(src, pad_idx, device):\n",
    "    src_seq_len = src.shape[1]\n",
    "\n",
    "    src_mask = generate_square_subsequent_mask(src_seq_len, device=device)\n",
    "\n",
    "    # NB: if you use this mask with hf models it should be !=, if vanilla torch => == \n",
    "    src_padding_mask = (src != pad_idx)\n",
    "    return src_mask, src_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import Phi3Config, Phi3ForCausalLM\n",
    "\n",
    "def calc_accuracy_and_hitrate_at_k(model: Phi3ForCausalLM, loader, device, k=3):\n",
    "    \n",
    "    total_seq_len_in_dataloader = 0\n",
    "    total_correct_predictions = 0\n",
    "    total_hits_in_loader = 0\n",
    "    \n",
    "    for batch in tqdm(loader):\n",
    "        src_mask, src_pad_mask = (create_masks(batch, pad_idx=loader.dataset.pad_id, device=device))\n",
    "        src_mask = src_mask.to(device)\n",
    "        src_pad_mask = src_pad_mask.to(device)\n",
    "        batch = batch.to(device)\n",
    "    \n",
    "        labels = batch[:, 1:]\n",
    "        labels_pad_mask = (labels == 0)\n",
    "        labels_without_pad = labels[~labels_pad_mask]\n",
    "        logits_without_last = model.forward(input_ids=batch, attention_mask=src_pad_mask).logits[:, :-1, :]\n",
    "        predictions = logits_without_last.argmax(dim=-1)\n",
    "        predictions_without_pad = predictions[~labels_pad_mask]\n",
    "\n",
    "        top_k_predictions = torch.argsort(logits_without_last, dim=-1, descending=True)[:, :, :k][~labels_pad_mask]\n",
    "\n",
    "    \n",
    "        total_seq_len_in_batch = labels_without_pad.shape[0]\n",
    "        total_predicions_len = predictions_without_pad.shape[0]\n",
    "    \n",
    "        assert total_seq_len_in_batch == total_predicions_len\n",
    "    \n",
    "        total_seq_len_in_dataloader += total_seq_len_in_batch\n",
    "    \n",
    "        correct_predictions = (labels_without_pad == predictions_without_pad).float().sum()\n",
    "\n",
    "        total_hits_in_batch = (top_k_predictions == labels_without_pad.unsqueeze(1)).any(dim=1).float().sum()\n",
    "        \n",
    "        total_hits_in_loader += total_hits_in_batch\n",
    "        total_correct_predictions += correct_predictions\n",
    "    \n",
    "    return (total_correct_predictions / total_seq_len_in_dataloader).item(), (total_hits_in_loader / total_seq_len_in_dataloader).item()\n",
    "\n",
    "def train_epoch(model: Phi3ForCausalLM, optimizer, loss_fn, train_dataloader: DataLoader, device):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "\n",
    "    for src in tqdm(train_dataloader, leave=False):\n",
    "        src = src.to(device)\n",
    "\n",
    "        src_mask, src_padding_mask = create_masks(src, pad_idx=train_dataloader.dataset.pad_id, device=device)\n",
    "\n",
    "        logits = model.forward(input_ids=src, attention_mask=src_padding_mask).logits[:, :-1, :]\n",
    "\n",
    "        src_out = src[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), src_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model: Phi3ForCausalLM, loss_fn, val_dataloader, device, k=3):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    for src in tqdm(val_dataloader, leave=False):\n",
    "        src = src.to(device)\n",
    "        src_mask, src_padding_mask = create_masks(src, pad_idx=val_dataloader.dataset.pad_id, device=device)\n",
    "\n",
    "        logits = model.forward(input_ids=src, attention_mask=src_padding_mask).logits[:, :-1, :]\n",
    "\n",
    "        src_out = src[:, 1:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), src_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    acc, hitrate = calc_accuracy_and_hitrate_at_k(model, val_dataloader, device, k)\n",
    "\n",
    "    return losses / len(val_dataloader), acc, hitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/921 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5138a5c7ce9e4b16b812aec4b7b5cf13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 1.477, Val loss: 1.410, Val ACC: 0.757, Val hitrate@3: 0.926, Epoch time = 89.823s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/921 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46aac829eb194fa3b13130c585ac560a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 1.384, Val loss: 1.388, Val ACC: 0.762, Val hitrate@3: 0.928, Epoch time = 89.555s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/921 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df18f30155af4514bf7381d90fd6112f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train loss: 1.366, Val loss: 1.377, Val ACC: 0.763, Val hitrate@3: 0.931, Epoch time = 89.708s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38dc416daea1489cb6883171d80e664f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/921 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, NUM_EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     51\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m timer()\n\u001b[0;32m---> 52\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m timer()\n\u001b[1;32m     54\u001b[0m     val_loss, val_acc, val_hitrate \u001b[38;5;241m=\u001b[39m evaluate(transformer, loss_fn, val_loader, device, k\u001b[38;5;241m=\u001b[39mk)\n",
      "Cell \u001b[0;32mIn[4], line 59\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, loss_fn, train_dataloader, device)\u001b[0m\n\u001b[1;32m     56\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     58\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), src_out\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 59\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     62\u001b[0m losses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from timeit import default_timer as timer\n",
    "from transformers import Phi3Config, Phi3ForCausalLM\n",
    "\n",
    "NUM_EPOCHS = 40\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = Phi3Config(\n",
    "    vocab_size=train_dataset.vocab_size,\n",
    "    hidden_size=256,\n",
    "    intermediate_size=1024,\n",
    "    num_hidden_layers=2,\n",
    "    num_attention_heads=4,\n",
    "    original_max_position_embeddings=512,\n",
    "    resid_pdrop=0.1,\n",
    "    embd_pdrop=0.1,\n",
    "    attention_dropout=0.1,\n",
    "    max_position_embeddings=512,\n",
    "    pad_token_id=train_dataset.pad_id,\n",
    "    bos_token_id=train_dataset.bos_id,\n",
    "    eos_token_id=train_dataset.eos_id,\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "model = Phi3ForCausalLM(config)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transformer = model.to(device)\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=train_dataset.pad_id, label_smoothing=0.07)\n",
    "\n",
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.01)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=1, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=1, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=1, pin_memory=True)\n",
    "k = 3\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer, loss_fn, train_loader, device)\n",
    "    end_time = timer()\n",
    "    val_loss, val_acc, val_hitrate = evaluate(transformer, loss_fn, val_loader, device, k=k)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, Val ACC: {val_acc:.3f}, Val hitrate@{k}: {val_hitrate:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.save_pretrained(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceee3e8df9b047b7ba5935d1c261be6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.7478238344192505, 0.9565130472183228)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_accuracy_and_hitrate_at_k(transformer, test_loader, device, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2216704"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in transformer.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SELECT': 4,\n",
       " 'Identifier': 5,\n",
       " 'FROM': 6,\n",
       " 'WHERE': 7,\n",
       " 'Equals': 8,\n",
       " 'QuotedIdentifier': 9,\n",
       " 'GROUP': 10,\n",
       " 'BY': 11,\n",
       " 'ORDER': 12,\n",
       " 'OpeningRoundBracket': 13,\n",
       " 'NAME': 14,\n",
       " 'ClosingRoundBracket': 15,\n",
       " 'DESC': 16,\n",
       " 'LIMIT': 17,\n",
       " 'Number': 18,\n",
       " 'Comma': 19,\n",
       " 'Asterisk': 20,\n",
       " 'DISTINCT': 21,\n",
       " 'HAVING': 22,\n",
       " 'Greater': 23,\n",
       " 'AND': 24,\n",
       " 'MAX': 25,\n",
       " 'SOURCE': 26,\n",
       " 'LIKE': 27,\n",
       " 'TYPE': 28,\n",
       " '*': 29,\n",
       " 'NO': 30,\n",
       " 'Dot': 31,\n",
       " 'AS': 32,\n",
       " 'JOIN': 33,\n",
       " 'ON': 34,\n",
       " 'YEAR': 35,\n",
       " 'UNION': 36,\n",
       " 'Slash': 37,\n",
       " 'MIN': 38,\n",
       " 'ID': 39,\n",
       " 'GreaterOrEquals': 40,\n",
       " '=': 41,\n",
       " 'BETWEEN': 42,\n",
       " 'Less': 43,\n",
       " 'Minus': 44,\n",
       " 'NotEquals': 45,\n",
       " 'MATCH': 46,\n",
       " 'Plus': 47,\n",
       " 'DIV': 48,\n",
       " 'EXTRACT': 49,\n",
       " 'NOT': 50,\n",
       " 'IN': 51,\n",
       " 'MONTH': 52,\n",
       " 'DAY': 53,\n",
       " 'StringLiteral': 54,\n",
       " 'Semicolon': 55,\n",
       " 'INTERSECT': 56,\n",
       " 'ASC': 57,\n",
       " 'OR': 58,\n",
       " 'DATE': 59,\n",
       " 'EVENTS': 60,\n",
       " 'EXCEPT': 61,\n",
       " '.': 62,\n",
       " 'CHARACTER': 63,\n",
       " 'LessOrEquals': 64,\n",
       " 'POSITION': 65,\n",
       " 'RANGE': 66,\n",
       " 'HOST': 67,\n",
       " 'ROLE': 68,\n",
       " 'ROLES': 69,\n",
       " 'EVENT': 70,\n",
       " 'TIES': 71,\n",
       " 'LIST': 72,\n",
       " 'GRANTS': 73,\n",
       " 'LEVEL': 74,\n",
       " 'VOLUME': 75,\n",
       " 'TABLE': 76,\n",
       " 'CURRENT': 77,\n",
       " 'FORMAT': 78,\n",
       " 'ENGINE': 79,\n",
       " 'NONE': 80,\n",
       " 'PRIMARY': 81,\n",
       " 'Percent': 82,\n",
       " 'TOP': 83,\n",
       " 'SECOND': 84,\n",
       " 'CASE': 85,\n",
       " 'Colon': 86,\n",
       " 'FOR': 87,\n",
       " 'LEADING': 88,\n",
       " 'TO': 89,\n",
       " 'WEEK': 90,\n",
       " 'SETTING': 91,\n",
       " 'TRUE': 92,\n",
       " 'IS': 93,\n",
       " 'MEMORY': 94,\n",
       " 'LOCAL': 95,\n",
       " 'TEST': 96,\n",
       " 'OpeningSquareBracket': 97,\n",
       " 'ClosingSquareBracket': 98,\n",
       " 'MI': 99,\n",
       " 'PARTIAL': 100,\n",
       " 'SALT': 101,\n",
       " 'TRIM': 102,\n",
       " 'At': 103,\n",
       " 'FINAL': 104,\n",
       " 'END': 105,\n",
       " 'FIRST': 106,\n",
       " 'COMPRESSION': 107,\n",
       " 'LAST': 108,\n",
       " 'GLOBAL': 109,\n",
       " 'COMMENT': 110,\n",
       " 'PART': 111,\n",
       " 'DollarSign': 112,\n",
       " 'GRANT': 113,\n",
       " 'LEFT': 114,\n",
       " 'FULL': 115,\n",
       " 'INDEX': 116,\n",
       " 'QUARTER': 117,\n",
       " 'MM': 118,\n",
       " 'LIVE': 119,\n",
       " 'CAST': 120,\n",
       " 'CHANGE': 121,\n",
       " 'QuestionMark': 122,\n",
       " 'WITH': 123,\n",
       " 'PROFILE': 124,\n",
       " 'AST': 125,\n",
       " 'RESET': 126,\n",
       " 'CROSS': 127,\n",
       " 'SETS': 128,\n",
       " 'SPATIAL': 129,\n",
       " 'PRECISION': 130,\n",
       " 'FOREIGN': 131,\n",
       " 'RIGHT': 132,\n",
       " 'EXCHANGE': 133,\n",
       " 'SERVER': 134,\n",
       " 'VIEW': 135,\n",
       " 'HH': 136,\n",
       " 'SS': 137,\n",
       " 'STEP': 138,\n",
       " 'DISK': 139,\n",
       " 'THEN': 140,\n",
       " 'ALL': 141,\n",
       " 'HOUR': 142,\n",
       " 'OPTION': 143,\n",
       " 'USE': 144,\n",
       " 'NEXT': 145,\n",
       " 'OVER': 146,\n",
       " 'CACHE': 147,\n",
       " 'MINUTE': 148,\n",
       " 'TREE': 149,\n",
       " 'POLICY': 150,\n",
       " 'NAMED': 151,\n",
       " 'WHEN': 152,\n",
       " 'SHOW': 153,\n",
       " 'CHAR': 154,\n",
       " 'FREEZE': 155,\n",
       " 'WINDOW': 156,\n",
       " 'SEMI': 157,\n",
       " 'AFTER': 158,\n",
       " 'PIPELINE': 159,\n",
       " 'FUNCTION': 160,\n",
       " 'INTERVAL': 161,\n",
       " 'KEY': 162,\n",
       " 'SYSTEM': 163,\n",
       " 'REALM': 164,\n",
       " 'DD': 165,\n",
       " 'YYYY': 166,\n",
       " 'ROW': 167,\n",
       " 'GROUPS': 168,\n",
       " 'TRIGGER': 169,\n",
       " 'DROP': 170,\n",
       " 'TOTALS': 171,\n",
       " 'SAMPLE': 172,\n",
       " 'MS': 173,\n",
       " 'LARGE': 174,\n",
       " 'EXTERNAL': 175,\n",
       " 'INTO': 176,\n",
       " 'IF': 177,\n",
       " 'STORAGE': 178,\n",
       " 'ONLY': 179,\n",
       " 'ESTIMATE': 180,\n",
       " 'PipeMark': 181,\n",
       " 'LAMBDA': 182,\n",
       " 'USER': 183,\n",
       " 'ANY': 184,\n",
       " 'FILTER': 185,\n",
       " 'ACCESS': 186,\n",
       " 'USING': 187,\n",
       " 'INNER': 188,\n",
       " 'LAYOUT': 189,\n",
       " 'GREATER': 190,\n",
       " 'ANTI': 191,\n",
       " 'WATCH': 192,\n",
       " 'CN': 193,\n",
       " 'CLEAR': 194,\n",
       " 'SET': 195,\n",
       " 'OBJECT': 196,\n",
       " 'BEGIN': 197,\n",
       " 'PLUS': 198,\n",
       " 'REFERENCES': 199,\n",
       " 'WK': 200,\n",
       " 'QUOTA': 201,\n",
       " '>': 202,\n",
       " 'Concatenation': 203,\n",
       " '<': 204,\n",
       " 'MUTATION': 205,\n",
       " 'LIMITS': 206,\n",
       " 'IP': 207,\n",
       " 'PLAN': 208,\n",
       " 'MILLISECOND': 209,\n",
       " 'TRACKING': 210,\n",
       " 'DATABASES': 211,\n",
       " 'LIFETIME': 212,\n",
       " 'PROJECTION': 213,\n",
       " 'RESTORE': 214,\n",
       " 'ALIAS': 215,\n",
       " 'COLLECTION': 216,\n",
       " 'ACTION': 217,\n",
       " 'CLUSTER': 218,\n",
       " 'FILE': 219,\n",
       " 'FOLLOWING': 220,\n",
       " 'TRAILING': 221,\n",
       " 'THAN': 222,\n",
       " 'OUTER': 223,\n",
       " 'BOTH': 224,\n",
       " 'SETTINGS': 225,\n",
       " 'SIMPLE': 226,\n",
       " 'UPDATE': 227,\n",
       " 'VALUES': 228,\n",
       " 'SIGNED': 229,\n",
       " 'CUBE': 230}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedTokenizerFast\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create a simple tokenizer\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mPreTrainedTokenizerFast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m vocab \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mtoken2idx  \u001b[38;5;66;03m# Reserve 0, 1, 2 for special tokens\u001b[39;00m\n\u001b[1;32m      7\u001b[0m vocab[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:134\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(2) a slow tokenizer instance to convert or \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(3) an equivalent slow tokenizer class to instantiate and convert. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m     )\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer \u001b[38;5;241m=\u001b[39m fast_tokenizer\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Create a simple tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=None)\n",
    "vocab = train_dataset.token2idx  # Reserve 0, 1, 2 for special tokens\n",
    "\n",
    "vocab[\"<pad>\"] = 0\n",
    "vocab[\"<bos>\"] = 1\n",
    "vocab[\"<eos>\"] = 2\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    vocab=vocab,\n",
    "    pad_token=\"<pad>\",\n",
    "    bos_token=\"<bos>\",\n",
    "    eos_token=\"<eos>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwargs option vocab\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Define the vocabulary\n",
    "vocab = train_dataset.token2idx  # Reserve 0, 1, 2 for special tokens\n",
    "\n",
    "vocab[\"<pad>\"] = 0\n",
    "vocab[\"<bos>\"] = 1\n",
    "vocab[\"<eos>\"] = 2  # Add all unique words\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token=\"<unk>\"))\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# Train the tokenizer\n",
    "trainer = trainers.WordLevelTrainer(vocab=vocab, special_tokens=[\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"])\n",
    "tokenizer.train_from_iterator(vocab, trainer)\n",
    "\n",
    "# Add post-processing\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"<bos> $A <eos>\",\n",
    "    pair=\"<bos> $A <eos> $B:1 <eos>:1\",\n",
    "    special_tokens=[\n",
    "        (\"<bos>\", 1),\n",
    "        (\"<eos>\", 2),\n",
    "    ],\n",
    ")\n",
    "tokenizer.decoder = decoders.WordPiece()\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save(\"custom_tokenizer.json\")\n",
    "\n",
    "# Load the tokenizer as a PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"custom_tokenizer.json\", pad_token=\"<pad>\", bos_token=\"<bos>\", eos_token=\"<eos>\", model_max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PreTrainedTokenizerBase.save_pretrained() missing 1 required positional argument: 'save_directory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: PreTrainedTokenizerBase.save_pretrained() missing 1 required positional argument: 'save_directory'"
     ]
    }
   ],
   "source": [
    "tokenizer.save_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 180, 101, 76, 101, 225, 101, 66, 167, 83, 25, 101, 145, 25, 101, 148, 132, 41, 50, 113, 139, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"SELECT Identifier FROM Identifier WHERE Identifier Equals QuotedIdentifier GROUP BY Identifier ORDER BY Identifier OpeningRoundBracket NAME ClosingRoundBracket DESC LIMIT Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PreTrainedTokenizerFast' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PreTrainedTokenizerFast' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "tokenizer.model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
