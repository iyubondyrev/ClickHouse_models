{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spider https://huggingface.co/datasets/spider\n",
    "import pandas as pd\n",
    "\n",
    "df_parquet = pd.read_parquet('/Users/ivanbondyrev/Downloads/train-00000-of-00001.parquet')\n",
    "df_parquet_test = pd.read_parquet('/Users/ivanbondyrev/Downloads/validation-00000-of-00001.parquet')\n",
    "\n",
    "with open('cleaned_data/Spider/spider_train.txt', \"w+\") as file:\n",
    "    for line in df_parquet['query']:\n",
    "        file.write(line + \"\\n\")\n",
    "\n",
    "with open('cleaned_data/Spider/spider_test.txt', \"w+\") as file:\n",
    "    for line in df_parquet_test['query']:\n",
    "        file.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WikiSQL https://www.kaggle.com/datasets/shahrukhkhan/wikisql?resource=download&select=validation.csv\n",
    "# NB: very bad dataset, maybe do not use it\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def write_to_file(name: str, df):\n",
    "    with open(f\"cleaned_data/WikiSQL/{name}.txt\", \"w+\") as file:\n",
    "        for line in df['sql']:\n",
    "            file.write(line + '\\n')\n",
    "\n",
    "df_test = pd.read_csv('/Users/ivanbondyrev/Downloads/test.csv')\n",
    "df_validation = pd.read_csv('/Users/ivanbondyrev/Downloads/validation.csv')\n",
    "df_train = pd.read_csv('/Users/ivanbondyrev/Downloads/train.csv')\n",
    "\n",
    "write_to_file('train', df_train)\n",
    "write_to_file('test', df_test)\n",
    "write_to_file('validation', df_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clickhouse generated \n",
    "\n",
    "import re\n",
    "with open('/Users/ivanbondyrev/Downloads/query_log.tsv', 'r', encoding='latin-1') as file, open(\"dataset_tests_pr_60095.txt\", \"w\") as f:\n",
    "    for line in file:\n",
    "        if line.split(\"\\t\")[1] != \"QueryFinish\":\n",
    "            continue\n",
    "        string = line.split(\"\\t\")[16].encode().decode('unicode_escape')\n",
    "        string = re.sub(r'\\s+', ' ', string)\n",
    "        if string == \"SELECT 1\":\n",
    "            continue\n",
    "        if string.startswith(\"--\"):\n",
    "            continue\n",
    "        f.write(string)\n",
    "        f.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split clickhouse generated to train/test/split\n",
    "import random\n",
    "\n",
    "filename = \"cleaned_data/Clickhouse/clickhouse_tests_pr_60095.txt\"\n",
    "train_filename = \"cleaned_data/Clickhouse/clickhouse_train_pr_60095.txt\"\n",
    "test_filename = \"cleaned_data/Clickhouse/clickhouse_test_pr_60095.txt\"\n",
    "val_filename = \"cleaned_data/Clickhouse/clickhouse_validation_pr_60095.txt\"\n",
    "with open(filename, \"r\") as f:\n",
    "    lines = list(set(f.readlines()))\n",
    "    random.shuffle(lines)\n",
    "\n",
    "    train_idx = int(len(lines) * 0.75)\n",
    "    test_idx = int(len(lines) * 0.95)\n",
    "    \n",
    "    with open(train_filename, \"w+\") as train_f:\n",
    "        train_f.writelines(lines[:train_idx])\n",
    "    \n",
    "    with open(test_filename, \"w+\") as test_f:\n",
    "        test_f.writelines(lines[train_idx:test_idx])\n",
    "\n",
    "    with open(val_filename, \"w+\") as val_f:\n",
    "        val_f.writelines(lines[test_idx:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KaggleDBQA https://github.com/Chia-Hsuan-Lee/KaggleDBQA\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "with open(\"cleaned_data/KaggleDBQA_combined.txt\", \"w+\") as combined_file:\n",
    "    for file_name_json in os.listdir(\"KaggleDBQA/examples\"):\n",
    "        if file_name_json[:-5].endswith(\"test\") or file_name_json[:-5].endswith(\"fewshot\"):\n",
    "            continue\n",
    "        with open(os.path.join(\"KaggleDBQA/examples\", file_name_json), 'r') as file_json, \\\n",
    "             open(\"cleaned_data/\" + file_name_json[:-5] + \".txt\", \"w+\") as file_txt:\n",
    "                data = json.load(file_json)\n",
    "                for elem in data:\n",
    "                    query: str = elem[\"query\"]\n",
    "                    file_txt.write(query + \"\\n\")\n",
    "                    combined_file.write(query + \"\\n\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all non-clickhouse data we need to replace \"\" with '' because clickhouse does not support \"\" for string literals\n",
    "import os\n",
    "\n",
    "for walk_obj in os.walk(\"cleaned_data\"):\n",
    "    if len(walk_obj[1]) != 0:\n",
    "        continue\n",
    "    folder_name = walk_obj[0]\n",
    "    for file_name in walk_obj[2]:\n",
    "        file_name = os.path.join(folder_name, file_name)\n",
    "        if file_name.lower().count(\"clickhouse\") > 0:\n",
    "            continue\n",
    "    \n",
    "        text = \"\"\n",
    "        with open(file_name, \"r\") as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        with open(file_name, \"w\") as f:\n",
    "            f.write(text.replace(\"\\\"\", \"\\'\"))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic_text_to_sql https://huggingface.co/datasets/gretelai/synthetic_text_to_sql/tree/main\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_parquet = pd.read_parquet('synthetic_text_to_sql_train.snappy.parquet')\n",
    "df_parquet_test = pd.read_parquet('synthetic_text_to_sql_test.snappy.parquet')\n",
    "\n",
    "\n",
    "df_train, df_validation = train_test_split(df_parquet, test_size=0.07, random_state=42)\n",
    "\n",
    "\n",
    "with open('cleaned_data/train/synth_train.txt', \"w+\") as file:\n",
    "    for line in df_train['sql']:\n",
    "        file.write(line + \"\\n\")\n",
    "\n",
    "\n",
    "with open('cleaned_data/test/synth_test.txt', \"w+\") as file:\n",
    "    for line in df_parquet_test['sql']:\n",
    "        file.write(line + \"\\n\")\n",
    "\n",
    "\n",
    "with open('cleaned_data/validation/synth_validation.txt', \"w+\") as file:\n",
    "    for line in df_validation['sql']:\n",
    "        file.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
